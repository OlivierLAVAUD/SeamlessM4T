#!/usr/bin/env python3
"""
SeamlessM4T T2TT - Text-to-Text Translation
Traduction textuelle sp√©cialis√©e avec auto-d√©tection GPU

Bas√© sur: https://huggingface.co/docs/transformers/model_doc/seamless_m4t_v2
"""

import gradio as gr
import torch
import logging
from transformers import AutoProcessor, SeamlessM4Tv2Model
import numpy as np
import os
from datetime import datetime

# Configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SeamlessM4T_T2TT:
    """Traduction textuelle (Text-to-Text)"""
    
    # Langues support√©es par le mod√®le SeamlessM4T v2 pour T2TT
    # Selon la documentation, T2TT supporte les m√™mes langues que S2TT
    T2TT_SUPPORTED_LANGUAGES = {
        "arb": "Arabic",
        "ben": "Bengali",
        "cat": "Catalan",
        "ces": "Czech",
        "cmn": "Mandarin Chinese",
        "cym": "Welsh",
        "dan": "Danish",
        "deu": "German",
        "eng": "English",
        "est": "Estonian",
        "fin": "Finnish",
        "fra": "French",
        "hin": "Hindi",
        "ind": "Indonesian",
        "ita": "Italian",
        "jpn": "Japanese",
        "kan": "Kannada",
        "kor": "Korean",
        "mlt": "Maltese",
        "nld": "Dutch",
        "pes": "Western Persian",
        "pol": "Polish",
        "por": "Portuguese",
        "ron": "Romanian",
        "rus": "Russian",
        "slk": "Slovak",
        "spa": "Spanish",
        "swe": "Swedish",
        "swh": "Swahili",
        "tam": "Tamil",
        "tel": "Telugu",
        "tgl": "Tagalog",
        "tha": "Thai",
        "tur": "Turkish",
        "ukr": "Ukrainian",
        "urd": "Urdu",
        "uzn": "Northern Uzbek",
        "vie": "Vietnamese"
    }
    
    # Toutes les langues support√©es par SeamlessM4T (pour r√©f√©rence)
    ALL_SUPPORTED_LANGUAGES = {
        "afr": "Afrikaans", "amh": "Amharic", "ara": "Arabic", "arb": "Arabic (Modern Standard)",
        "asm": "Assamese", "aze": "Azerbaijani", "bel": "Belarusian", "ben": "Bengali",
        "bos": "Bosnian", "bul": "Bulgarian", "cat": "Catalan", "ceb": "Cebuano",
        "ces": "Czech", "ckb": "Central Kurdish", "cmn": "Mandarin Chinese", "cym": "Welsh",
        "dan": "Danish", "deu": "German", "ell": "Greek", "eng": "English",
        "est": "Estonian", "eus": "Basque", "fin": "Finnish", "fra": "French",
        "gaz": "West Central Oromo", "gle": "Irish", "glg": "Galician", "guj": "Gujarati",
        "heb": "Hebrew", "hin": "Hindi", "hrv": "Croatian", "hun": "Hungarian",
        "hye": "Armenian", "ibo": "Igbo", "ind": "Indonesian", "isl": "Icelandic",
        "ita": "Italian", "jav": "Javanese", "jpn": "Japanese", "kan": "Kannada",
        "kat": "Georgian", "kaz": "Kazakh", "kea": "Kabuverdianu", "khk": "Halh Mongolian",
        "khm": "Khmer", "kir": "Kyrgyz", "kor": "Korean", "lao": "Lao",
        "lit": "Lithuanian", "ltz": "Luxembourgish", "lug": "Ganda", "luo": "Luo",
        "lvs": "Standard Latvian", "mai": "Maithili", "mal": "Malayalam", "mar": "Marathi",
        "mkd": "Macedonian", "mlt": "Maltese", "mni": "Meitei", "mya": "Burmese",
        "nld": "Dutch", "nno": "Norwegian Nynorsk", "nob": "Norwegian Bokm√•l",
        "npi": "Nepali", "nya": "Nyanja", "oci": "Occitan", "ory": "Odia",
        "pan": "Punjabi", "pbt": "Southern Pashto", "pes": "Western Persian",
        "pol": "Polish", "por": "Portuguese", "ron": "Romanian", "rus": "Russian",
        "slk": "Slovak", "slv": "Slovenian", "sna": "Shona", "snd": "Sindhi",
        "som": "Somali", "spa": "Spanish", "srp": "Serbian", "swe": "Swedish",
        "swh": "Swahili", "tam": "Tamil", "tel": "Telugu", "tgk": "Tajik",
        "tgl": "Tagalog", "tha": "Thai", "tur": "Turkish", "ukr": "Ukrainian",
        "urd": "Urdu", "uzb": "Uzbek", "uzn": "Northern Uzbek", "vie": "Vietnamese",
        "xho": "Xhosa", "yid": "Yiddish", "yor": "Yoruba", "yue": "Cantonese",
        "zho": "Chinese", "zsm": "Standard Malay", "zul": "Zulu"
    }
    
    def __init__(self):
        """Initialisation avec auto-d√©tection GPU"""
        logger.info("üîã Initialisation de SeamlessM4T T2TT...")
        
        # Auto-d√©tection GPU/CPU
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üî• Utilisation du device: {self.device}")
        
        # Chargement du mod√®le
        self._load_model()
        
        # Initialiser le compteur de requ√™tes pour la gestion m√©moire
        self.request_count = 0
        self.MAX_REQUESTS_BEFORE_CLEANUP = 5  # Nettoyer apr√®s 5 requ√™tes
    
    def _cleanup_gpu_memory(self):
        """Nettoyer la m√©moire GPU de mani√®re s√ªre"""
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                # Vider le cache CUDA
                torch.cuda.empty_cache()
                
                # Synchroniser pour s'assurer que le nettoyage est termin√©
                torch.cuda.synchronize()
                
                # V√©rifier la m√©moire disponible
                free_memory = torch.cuda.mem_get_info()[0] / 1024**3  # en Go
                logger.info(f"üßπ M√©moire GPU nettoy√©e. Disponible: {free_memory:.1f} Go")
                
                return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Impossible de nettoyer la m√©moire GPU: {e}")
        return False
    
    def _load_model(self):
        """Chargement optimis√© du mod√®le sans avertissement sampling_rate"""
        try:
            logger.info("üîã Chargement du mod√®le SeamlessM4T pour T2TT...")
            
            # Cr√©er le processeur pour T2TT
            self.processor = AutoProcessor.from_pretrained(
                "facebook/seamless-m4t-v2-large",
                use_fast=False
            )
            logger.info("‚úÖ Processeur charg√©")
            
            # Charger le mod√®le avec optimisation
            model_kwargs = {
                "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32,
                "low_cpu_mem_usage": True
            }
            
            self.model = SeamlessM4Tv2Model.from_pretrained(
                "facebook/seamless-m4t-v2-large",
                **model_kwargs
            ).to(self.device)
            
            logger.info("‚úÖ Mod√®le charg√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur de chargement: {e}")
            raise
    
    def translate_text(self, text: str, src_lang: str, tgt_lang: str) -> str:
        """Traduction textuelle (T2TT) avec gestion des erreurs"""
        try:
            # Incr√©menter le compteur de requ√™tes
            self.request_count += 1
            
            # Nettoyer la m√©moire GPU p√©riodiquement
            if self.device == "cuda" and self.request_count >= self.MAX_REQUESTS_BEFORE_CLEANUP:
                self._cleanup_gpu_memory()
                self.request_count = 0  # R√©initialiser le compteur
            
            # Validation
            if src_lang not in self.ALL_SUPPORTED_LANGUAGES:
                raise ValueError(f"Langue source non support√©e: {src_lang}")
            if tgt_lang not in self.T2TT_SUPPORTED_LANGUAGES:
                supported_t2tt_langs = ", ".join(self.T2TT_SUPPORTED_LANGUAGES.keys())
                raise ValueError(f"Langue cible non support√©e pour T2TT: {tgt_lang}. "
                               f"Langues support√©es: {supported_t2tt_langs}")
            
            # Validation du texte
            if not text or not text.strip():
                raise ValueError("Le texte √† traduire ne peut pas √™tre vide")
            
            if len(text) > 10000:  # Limite pour √©viter les probl√®mes m√©moire
                logger.warning(f"‚ö†Ô∏è  Texte tr√®s long ({len(text)} caract√®res). D√©coupage en segments...")
                # D√©couper le texte en segments plus petits
                segment_length = 5000  # Environ 5000 caract√®res par segment
                segments = [text[i:i+segment_length] for i in range(0, len(text), segment_length)]
                translated_segments = []
                
                for i, segment in enumerate(segments):
                    logger.info(f"Traitement segment {i+1}/{len(segments)}")
                    try:
                        segment_translation = self._translate_single_text(segment, src_lang, tgt_lang)
                        translated_segments.append(segment_translation)
                    except Exception as segment_error:
                        logger.error(f"‚ùå Erreur de traduction du segment {i+1}: {segment_error}")
                        translated_segments.append(f"[ERREUR: {str(segment_error)}]")
                
                # Concatenation des traductions
                return " ".join(translated_segments)
            else:
                # Traitement normal pour les textes courts
                return self._translate_single_text(text, src_lang, tgt_lang)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur de traduction: {e}")
            # Toujours vider la m√©moire GPU en cas d'erreur
            self._cleanup_gpu_memory()
            raise
    
    def _translate_single_text(self, text: str, src_lang: str, tgt_lang: str) -> str:
        """Traduction d'un seul texte"""
        try:
            # Pr√©parer les entr√©es pour le mod√®le (T2TT utilise text_inputs)
            inputs = self.processor(
                text=[text],
                src_lang=src_lang,
                return_tensors="pt"
            ).to(self.device)
            
            # G√©n√©rer la traduction
            with torch.no_grad():
                try:
                    # Pour T2TT avec SeamlessM4T v2, nous devons utiliser la bonne approche
                    # Selon la documentation, pour T2TT nous devons:
                    # 1. D√©sactiver la g√©n√©ration audio
                    # 2. Utiliser la t√¢che appropri√©e
                    output = self.model.generate(
                        **inputs,
                        tgt_lang=tgt_lang,
                        generate_speech=False  # D√©sactiver la g√©n√©ration audio
                    )
                except Exception as model_error:
                    error_msg = str(model_error)
                    if "not supported by this model" in error_msg:
                        start_idx = error_msg.find("in ") + 3
                        end_idx = error_msg.find(". Note that")
                        if start_idx > 0 and end_idx > 0:
                            supported_langs = error_msg[start_idx:end_idx].strip()
                            raise ValueError(f"Langue cible '{tgt_lang}' non support√©e pour T2TT. "
                                           f"Langues support√©es: {supported_langs}")
                    raise
            
            # Extraire le texte g√©n√©r√© pour T2TT
            # Selon la documentation SeamlessM4T v2, la sortie est un objet sp√©cifique
            
            # Pour T2TT, la sortie devrait √™tre un objet avec les tokens de texte
            if hasattr(output, 'sequences'):
                # Cas 1: La sortie a un attribut 'sequences'
                generated_tokens = output.sequences
            elif isinstance(output, tuple):
                # Cas 2: La sortie est un tuple
                generated_tokens = output[0]
            elif hasattr(output, 'cpu'):
                # Cas 3: La sortie est un tensor
                generated_tokens = output.cpu()
            else:
                # Cas 4: Autre format
                generated_tokens = output
            
            # Convertir en texte
            try:
                # Utiliser batch_decode qui g√®re les diff√©rents formats
                text = self.processor.tokenizer.batch_decode(
                    generated_tokens,
                    skip_special_tokens=True
                )
                
                # Prendre le premier √©l√©ment si c'est une liste
                if isinstance(text, list) and len(text) > 0:
                    text = text[0]
                elif isinstance(text, list):
                    text = ""
                
                logger.info(f"Texte traduit: {text}")
                return text
            except Exception as e:
                logger.error(f"Erreur de d√©codage: {e}")
                raise ValueError(f"Impossible de d√©coder la sortie: {e}")
            finally:
                # Toujours vider la m√©moire GPU apr√®s le traitement
                if self.device == "cuda":
                    self._cleanup_gpu_memory()
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur de traduction textuelle: {e}")
            raise


class SeamlessT2TTApp:
    """Interface Gradio pour SeamlessM4T T2TT"""
    
    def __init__(self):
        self.t2tt = SeamlessM4T_T2TT()
        self.languages = self.t2tt.T2TT_SUPPORTED_LANGUAGES
    
    def t2tt_interface(self, text: str, src_lang: str, tgt_lang: str) -> str:
        """Interface pour la traduction textuelle"""
        try:
            translation_output = self.t2tt.translate_text(text, src_lang, tgt_lang)
            return translation_output
        except ValueError as e:
            error_msg = str(e)
            if len(error_msg) > 200:
                if "Langues support√©es:" in error_msg:
                    start_idx = error_msg.find("Langues support√©es:")
                    return error_msg[:start_idx + 20] + "... (voir logs)"
                else:
                    return error_msg[:200] + "..."
            return f"‚ùå {error_msg}"
        except Exception as e:
            error_msg = str(e)
            if len(error_msg) > 200:
                return f"‚ùå {error_msg[:200]}..."
            return f"‚ùå Erreur inattendue: {error_msg}"
        finally:
            # Toujours vider la m√©moire GPU apr√®s chaque requ√™te
            if hasattr(self.t2tt, '_cleanup_gpu_memory'):
                self.t2tt._cleanup_gpu_memory()
    
    def create_interface(self):
        """Cr√©e l'interface Gradio"""
        
        with gr.Blocks(title="SeamlessM4T T2TT") as app:
            gr.Markdown("""
            # üìù SeamlessM4T Text-to-Text Translation (T2TT)
            Traduction textuelle sp√©cialis√©e avec auto-d√©tection GPU
            
            **Fonctionnalit√©s:**
            - üìù Texte vers Texte (T2TT)
            - üî• Auto-d√©tection GPU/CPU
            - üåç Support multilingue (36 langues)
            - üìÑ Gestion des textes longs (d√©coupage automatique)
            - üßπ Nettoyage automatique de la m√©moire GPU
            
            **Langues support√©es pour T2TT:** Arabic, Bengali, Catalan, Czech, Mandarin, Welsh, Danish, German, English, Estonian, Finnish, French, Hindi, Indonesian, Italian, Japanese, Kannada, Korean, Maltese, Dutch, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Spanish, Swedish, Swahili, Tamil, Telugu, Tagalog, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese
            
            """)
            
            with gr.Row():
                t2tt_text = gr.Textbox(
                    label="Texte √† traduire",
                    lines=5,
                    placeholder="Entrez le texte √† traduire ici..."
                )
            
            with gr.Row():
                t2tt_src_lang = gr.Dropdown(
                    choices=list(self.languages.keys()),
                    value="fra",
                    label="Langue source"
                )
                t2tt_tgt_lang = gr.Dropdown(
                    choices=list(self.languages.keys()),
                    value="eng",
                    label="Langue cible"
                )
            
            t2tt_btn = gr.Button("Traduire texte", variant="primary")
            t2tt_output = gr.Textbox(label="Texte traduit", lines=5)
            
            t2tt_btn.click(
                fn=self.t2tt_interface,
                inputs=[t2tt_text, t2tt_src_lang, t2tt_tgt_lang],
                outputs=t2tt_output
            )
            
            gr.Markdown("""
            ---
            ### Informations
            - **Device:** " + ("üî• GPU" if torch.cuda.is_available() else "‚ùÑÔ∏è CPU") + ""
            - **Mod√®le:** facebook/seamless-m4t-v2-large
            - **Dur√©e max par segment:** 5000 caract√®res
            - **Nettoyage GPU:** Apr√®s 5 requ√™tes
            
            ¬© 2024 SeamlessM4T T2TT
            """)
        
        return app
    
    def launch(self):
        """Lance l'application"""
        try:
            app = self.create_interface()
            app.launch(
                server_name="0.0.0.0",
                server_port=7869,  # Port diff√©rent des autres applications
                share=False
            )
        finally:
            # Nettoyer les ressources GPU √† la fin
            if hasattr(self.t2tt, '_cleanup_gpu_memory'):
                self.t2tt._cleanup_gpu_memory()
            logger.info("üßπ Ressources GPU nettoy√©es √† la fin de l'application")


if __name__ == "__main__":
    try:
        logger.info("üöÄ Lancement de SeamlessM4T T2TT...")
        app = SeamlessT2TTApp()
        app.launch()
    except Exception as e:
        logger.error(f"Erreur fatale: {e}")
        # Nettoyer les ressources GPU en cas d'erreur fatale
        if 'app' in locals() and hasattr(app.t2tt, '_cleanup_gpu_memory'):
            app.t2tt._cleanup_gpu_memory()
        raise